{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (EM) Algorithm Applied to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial summarizes how the EM algorithm can be applied to texts. It is based on Stephen Hansen's lecture notes <a>https://sekhansen.github.io/pdf_files/lecture2.pdf</a> and text-mining module <a>https://github.com/sekhansen/text-mining-tutorial<a/>.\n",
    "\n",
    "[**Just Give Me the Code**](#gimmethecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, suppose we have 3 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [[\"rugby\",\"football\",\"competition\",\"ball\",\"games\"],\n",
    "        [\"macro\",\"economics\",\"io\",\"competition\",\"games\",\"econometrics\"],\n",
    "        [\"business\",\"economics\",\"stocks\",\"bonds\",\"NYSE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, document 1 is a <i>sports</i> document, document 2 is related to <i>economics</i>, and document 3 is related to <i>business</i>. \n",
    "\n",
    "**Goal**\n",
    "1. Topic Distribution <br>\n",
    "The goal is to find the distribution of K topics. That is, in our corpus what is the probability a random document is related to topic 1 or topic 2 or ... topic K? This is represented by $\\rho$=$(\\rho_1,\\rho_2,...\\rho_K)$ where $1\\geq\\rho_k\\geq0$. This is just a vector of size K. \n",
    "<br>\n",
    "<br>\n",
    "2. Word Distrubition <br>\n",
    "Given a topic $k$, what is the probability that the document contains any word in our vocabulary of size V. This is represented by $\\mathbf\\beta_k$=($\\beta_{1,k},\\beta_{2,k},...,\\beta_{V,k})$ where $1\\geq\\beta_{v,k}\\geq0$. We will have K such vectors, so $\\mathbf\\beta$ will be a matrix of size K by V.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We will assume that the probability of observing a document $w_d$ is: $Pr(w_d | \\rho, \\mathbf\\beta)=\\Sigma_{k=1}^V(\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})$, where $x_{v,d}$ is the occurences of word v in document d.\n",
    "\n",
    "\n",
    "So the probability of observing document 1 is just: <br>$Pr$([\"rugby\",\"football\",\"competition\",\"ball\",\"games\"] | $\\rho, \\mathbf\\beta)=\\rho_{1}(\\beta_{ball,1}^{1}*\\beta_{bonds,1}^{0}*...)+\\rho_{2}(\\beta_{ball,2}^{1}*\\beta_{bonds,2}^{0}*...)$\n",
    "\n",
    "\n",
    "We can represent the data in a frequency matrix where each row is a document, and each column is a word the vocabulary. The values correspond to the frequencies a word shows up in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ball  bonds  business  competition  economics  football  games  macro  rugby  stocks\n",
      "0     1      0         0            1          0         1      1      0      1       0\n",
      "1     0      0         0            1          1         0      1      1      0       0\n",
      "2     0      1         1            0          1         0      0      0      0       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "feature_counts = pd.DataFrame(data=\n",
    "                [[ 1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.],                            \n",
    "                 [ 0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.],\n",
    "                 [ 0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]], \n",
    "                columns=[u'ball', u'bonds', u'business', u'competition', u'economics',                   \n",
    "                         u'football', u'games', u'macro', u'rugby', u'stocks'])\n",
    "\n",
    "print(feature_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM algorithm starts by guessing the probability that a document comes from topic $k$. Let's define the number of topics, K, and our initial guess of the distribution of each topic.\n",
    "\n",
    "We import numpy as this module gives us a lot of useful functions like linear algebra operations and sampling from different types of distributions. \n",
    "\n",
    "<b>k</b> is the number of (unknown or latent) topics.\n",
    "\n",
    "<b>rhos</b> ($\\rho$) is the initial \"guess\" of the distribution of topics. The uniform distribution is a good starting point. We'll guess the probability of a document belonging to topic 1 is 50% and topic 2 is 50%. \n",
    "\n",
    "<b>betas</b> ($\\mathbf\\beta$) we will also guess , the probability matrix where word $v$ appears in topic $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = 2\n",
    "\n",
    "D = feature_counts.shape[0]\n",
    "\n",
    "V = feature_counts.shape[1]\n",
    "\n",
    "rhos = np.full(k,1/float(k))\n",
    "\n",
    "betas = np.random.dirichlet(V*[1], K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-Step###\n",
    "\n",
    "First let's see how much each document likes each topic. We do this by computing each documents likelihood, given our initial guess of $\\rho$ and $\\beta$. We will log these likelihoods then exponentiate them later.\n",
    "\n",
    "Log-Likelihood of document $w_d$:\n",
    "$log(Pr(w_d | \\rho, \\mathbf\\beta))=\\Sigma_{k=1}^K(log(\\rho_k)+ \\Sigma_{v=1}^Vlog(\\beta_{v,k})*{x_{v,d}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_temp = np.zeros((D,k))\n",
    "for doc in range(D):\n",
    "    for topic in range(k):\n",
    "        dt_temp[doc,topic] = np.log(rhos[topic]) + (feature_counts.values[doc]*np.log(betas[topic])).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in the log world so we need to un-log, or exponentiate, the likelihoods to retrieve $\\Sigma_{k=1}(\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.47750913e-06   1.11585535e-05]\n",
      " [  4.05101144e-07   8.74903957e-06]\n",
      " [  1.74182527e-06   4.72287999e-07]]\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(dt_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the extremely small numbers. We will eventually need to calculate the complete likelihood over all documents, that is $l(X|\\rho\\beta) = \\Sigma_{d=1}^{D} log(\\Sigma_{k=1}^K\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}})=\\Sigma_{d=1}^{D} log(\\Sigma_{k=1}^Ke^{(log(\\rho_k)+ \\Sigma_{v=1}^Vlog(\\beta_{v,k})*{x_{v,d}})})$. Taking the logarithms of such small numbers, as in the print above, will lead to <b>numerical underflow</b>. Fortunately, we can use the <b>log-sum trick</b>, which is factoring out the largest exponent:\n",
    "\n",
    "$log\\Sigma_{c}e^{b_c} = log[(\\Sigma_{c}e^{b_c-B})e^B]=[log(\\Sigma_ce^{b_c-B})]+B$, where $B$ is the largest $b_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.02184869  0.        ]\n",
      " [-3.07256244  0.        ]\n",
      " [ 0.         -1.30509988]]\n",
      "-35.9009185218\n"
     ]
    }
   ],
   "source": [
    "dt_tempZ = dt_temp - np.max(dt_temp,1)[:,np.newaxis]\n",
    "\n",
    "print(dt_tempZ)\n",
    "\n",
    "ll = np.log(np.exp(dt_tempZ).sum(1)).sum()+dt_temp.max(1).sum()\n",
    "\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-Step###\n",
    "\n",
    "Now we need to update our initial guesses of $\\rho$ and $\\beta$. \n",
    "\n",
    "1. First, let's update $\\rho$. \n",
    "\n",
    "We will use the formula:\n",
    "\n",
    "$\\rho_k^{new}=\\frac{\\Sigma_d\\hat{z_{d,k}}}{\\Sigma_k\\Sigma_d\\hat{z_{d,k}}}$ where $\\hat{z_{d,k}}=\\frac{\\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}}}{\\Sigma_d \\rho_k \\Pi_{v=1}^V\\beta_{v,k}^{x_{v,d}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11692797  0.88307203]\n",
      " [ 0.04425332  0.95574668]\n",
      " [ 0.78669203  0.21330797]]\n"
     ]
    }
   ],
   "source": [
    "probs = np.exp(dt_temp)/np.exp(dt_temp).sum(1)[:,np.newaxis]\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row shows the distribution of a document across the K topics, where the rows sum to 1. For example, the top left element is how much [\"rugby\",\"football\",\"competition\",\"ball\",\"games\"] likes topic 1. \n",
    "\n",
    "We can just average each column across rows to come up with the new values of $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31595777  0.68404223]\n"
     ]
    }
   ],
   "source": [
    "rhos = probs.sum(0)/D\n",
    "\n",
    "print(rhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Second, let's update $\\beta$\n",
    "\n",
    "We will use the formula \n",
    "$\\beta_{k,v}^{new}=\\frac{\\Sigma_d\\hat{z_{d,k}}*x_{d,v}}{\\Sigma_d\\hat{z_{d,k}}*\\Sigma_vx_{d,v}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02991693  0.20128128  0.20128128  0.04123949  0.21260384  0.02991693\n",
      "   0.04123949  0.01132256  0.02991693  0.20128128]\n",
      " [ 0.09713077  0.02346215  0.02346215  0.20225516  0.12858654  0.09713077\n",
      "   0.20225516  0.10512439  0.09713077  0.02346215]]\n"
     ]
    }
   ],
   "source": [
    "betas_temp = np.dot(probs.T,feature_counts.values)\n",
    "betas=betas_temp/betas_temp.sum(1)[:,np.newaxis]\n",
    "\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's one iteration of the algorithm. The E-step and the M-step would be repeated until the log-likehood increases very litle. We can implement the entire code in the form of a class as follows:\n",
    "\n",
    "<a id='gimmethecode'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "### Author: Paul Soto \t\t  ###\n",
    "### \t\tpaul.soto@upf.edu ###\n",
    "#\t\t\t\t\t\t\t\t#\n",
    "# This file is a class to #######\n",
    "# perform the EM algorithm ######\n",
    "# on a multinomial distribution##\n",
    "#################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EM_algo():\n",
    "\t\"\"\"\"\n",
    "\tAlgorithm for the multinomial mixture model\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, data, K=2,\n",
    "\t\t\t\t\tprior_rhos=None, \n",
    "\t\t\t\t\tprior_betas=None):\n",
    "\t\tself.D = data.shape[0]\n",
    "\t\tself.V = data.shape[1]\n",
    "\t\tself.K = K\n",
    "\t\tself.data = data\n",
    "\t\tif prior_rhos==None:\n",
    "\t\t\tself.rhos = np.full(self.K, 1/float(self.K))\n",
    "\t\telse:\n",
    "\t\t\tself.rhos = prior_rhos\n",
    "\t\tif prior_betas==None:\n",
    "\t\t\tself.betas = np.random.dirichlet(self.V*[1], self.K)\n",
    "\t\telse:\n",
    "\t\t\tself.betas =prior_betas \n",
    "\t\tself.ll_history = []\n",
    "\tdef E_step(self):\n",
    "\t\t### Get likelihood for each document-topic\n",
    "\t\tdt_temp = np.zeros((self.D,self.K))\n",
    "\t\tfor doc in range(self.D):\n",
    "\t\t\tfor topic in range(self.K):\n",
    "\t\t\t\tdt_temp[doc,topic] = np.log(self.rhos[topic]) + \\\n",
    "\t\t\t\t\t\t\t\t\t(self.data[doc]*np.log(self.betas[topic])).sum()\n",
    "\n",
    "\n",
    "\t\t### Use log-sum trick\n",
    "\t\tdt_tempZ = dt_temp - np.max(dt_temp,1)[:,np.newaxis]\n",
    "\n",
    "\t\t### Compute complete log-likelihood\n",
    "\t\tll = np.log(np.exp(dt_tempZ).sum(1)).sum()+dt_temp.max(1).sum()\n",
    "\t\t### normalize the Zs\n",
    "\t\tself.zs = np.exp(dt_temp)/np.exp(dt_temp).sum(1)[:,np.newaxis]\n",
    "\t\tself.ll = ll\n",
    "\t\tself.ll_history.append(ll)\n",
    "\n",
    "\tdef M_step(self):\n",
    "\t\t### Update rhos\n",
    "\t\tself.rhos = self.zs.sum(0)/self.D\n",
    "\t\t### Update betas\n",
    "\t\tbetas_temp = np.dot(self.zs.T,self.data)\n",
    "\t\tself.betas=betas_temp/betas_temp.sum(1)[:,np.newaxis]\n",
    "\n",
    "\tdef iterate(self,tolerance=0.001,max_iter = 500):\n",
    "\t\tcount = 0\n",
    "\t\twhile True:\n",
    "\t\t\tself.E_step()\n",
    "\t\t\tself.M_step()\n",
    "\t\t\tif len(self.ll_history)==1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif (self.ll-self.ll_history[-2])<tolerance:\n",
    "\t\t\t\tprint(\"Arrived at target tolerance!\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif (count>max_iter):\n",
    "\t\t\t\tprint(\"Exceeded max_iterations!\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\tcount+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrived at target tolerance!\n",
      "[ 0.33333333  0.66666667]\n",
      "[[  0.00000000e+000   2.50000000e-001   2.50000000e-001   3.18096923e-141\n",
      "    2.50000000e-001   0.00000000e+000   3.18096923e-141   3.18096923e-141\n",
      "    0.00000000e+000   2.50000000e-001]\n",
      " [  1.11111111e-001   6.39822930e-091   6.39822930e-091   2.22222222e-001\n",
      "    1.11111111e-001   1.11111111e-001   2.22222222e-001   1.11111111e-001\n",
      "    1.11111111e-001   6.39822930e-091]]\n"
     ]
    }
   ],
   "source": [
    "em_instance = EM_algo(feature_counts.values,K=2)\n",
    "\n",
    "em_instance.iterate()\n",
    "\n",
    "print(em_instance.rhos)\n",
    "print(em_instance.betas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
