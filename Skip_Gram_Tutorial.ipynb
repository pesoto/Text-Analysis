{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram for Word Embeddings (Word Vectors) Tutorial\n",
    "\n",
    "This tutorial walks through the Skip-Gram model proposed by [Mikolov et al](https://arxiv.org/pdf/1301.3781.pdf). The purpose of this tutorial is more about the architecture of the model rather than the estimation. \n",
    "\n",
    "<b>IMPORTANT</b>: Here, I only discuss training the model with gradient descent, however as practitioners will know, this is inefficient with large databases. As a result, [Mikolov et al](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) released another paper with a technique known as negative sampling, which trains only a subset of weights each epoch. This would be more effective in obtaining word vectors with an actual text database. This will be the topic of a subsequent tutorial.\n",
    "\n",
    "## Example ##\n",
    "Textual analysis is a booming field with practical applications in social science. The first thing to consider in most textual analyses is the representation of words in a computer program. \n",
    "\n",
    "Consider the following sentences:\n",
    "\n",
    "Example 1: <i>we think uncertainty about unemployment</i><br>\n",
    "Example 2: <i>uncertainty and fears about inflation</i><br>\n",
    "Example 3: <i>we think fears about unemployment</i><br>\n",
    "Example 4: <i>we think fears and uncertainty about inflation and unemployment</i><br>\n",
    "Example 5: <i>constant negative press covfefe</i><br>\n",
    "\n",
    "In total, there are 12 unique words: and, about, constant, unemployment, uncertainty, fears, we, negative, inflation, press, think, covfefe\n",
    "\n",
    "## One-Hot Representation ##\n",
    "One common way of representing words is with a <b>one-hot encoded</b> vector. This would be a vector of length 12 where each index represents one of the 12 unique words. We can represent each word with such a vector, with a 1 at the index of the word and zero everywhere else. \n",
    "\n",
    "For example, we can represent the word <i>fears</i> as follows:\n",
    "$$\n",
    "\\small{\\begin{array}{|cccccccccccc|}\n",
    " \\text{and}  & \\text{uncertainty} & \\text{fears}& \\text{we}& \\text{about}& \\text{constant}& \\text{unemployment}& \\text{negative}& \\text{inflation}& \\text{press}& \\text{think}& \\text{covfefe} \\\\\n",
    "\\hline\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{array}}$$\n",
    "\n",
    "### Why use word vectors/embeddings? ###\n",
    "One issue with this representation is how to relate two words. How is the word <i>fears</i> related to the word <i>uncertainty</i>? They both contain length 12, have a 1 only in one index and 0 everywhere else. Essentially, all individual words are the same distance from each other.\n",
    "\n",
    "$$\\small{\\begin{array}{l|cccccccccccc|}\n",
    "& \\text{and}  & \\text{uncertainty} & \\text{fears}& \\text{we}& \\text{about}& \\text{constant}& \\text{unemployment}& \\text{negative}& \\text{inflation}& \\text{press}& \\text{think}& \\text{covfefe} \\\\\n",
    "\\hline\n",
    "\\text{fears} & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\hline\n",
    "\\text{uncertainty} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{array}}$$\n",
    "\n",
    "Word vectors, or [word embeddings](https://en.wikipedia.org/wiki/Word_embedding), solve this issue. They map words to some arbitrary H-dimensional space, $\\mathbb{R}^H$, where semantic and syntactic similarities are preserved. Below provides an example of these 12 words mapped into a 3-Dimensional space. The 3-Dimensional vectors were obtained using the Skip-Gram model proposed by Mikolov et al. Notice, that words <i>fear</i> and <i>uncertainty</i> appear alongisde each, as do the words <i>inflation</i> and <i>unemployment</i>, suggesting similar meanings (with respect to their usage in sentences) through syntax and semantics. \n",
    "\n",
    "<img src=\"markdown/scatter.png\">\n",
    "\n",
    "## Overview of Skip-Gram Model to Obtain Word Embeddings ##\n",
    "\n",
    "The Skip-Gram Model aims to understand the link between words in sentences, and their context. For example, in Example 1, <i>we think uncertainty about unemployment</i>, consider the word <i>uncertainty</i>. The words within it's immediate proximity are <i>think</i> and <i>about</i>. Word embeddings leverage the information that similar words typically show up with similar contexts. \n",
    "\n",
    "The Skip-Gram Model aims to use as input the one-hot encoded vector for the word <i>uncertainty</i> and output the word <i>think</i> and the word <i>about</i>, each represented as their own one-hot encoded vector. Let's take the input <i>uncertainty</i> and output <i>think</i>. Below is an image of the neural network at work (it might look daunting at first, so I'll try to explain each step below!).\n",
    "\n",
    "### Example of 1 Training Example: Input <i>uncertainty</i>, Output <i>think</i>###\n",
    "<img src=\"markdown/neural_net.png\">\n",
    "\n",
    "Each step in the neural network is known as a layer. \n",
    "\n",
    "###Input Layer### \n",
    "Here we define the input, represented by a 12 length vector as above, with a one at the target word, <i>uncertainty</i>.\n",
    "\n",
    "###Hidden Layer###\n",
    "This is the target word represented in the H-dimensional space, $\\mathbb{R}^H$. In our example, we will use a 3-Dimensional space.\n",
    "\n",
    "How do we get the Hidden Layer from the Input Layer of just a one-hot encoded vector. We multiply the Input Vector by a 3-by-12 matrix <b>we will call the $\\mathbf{V}$ matrix</b>.\n",
    "\n",
    "$x*\\mathbf{V}^T=\\begin{array}{cccccc}[0&0&1&...&0&0]\\end{array}*\\left(  \\begin{array}{cccc}\n",
    "v_{1,1} & v_{1,2} & ... & v_{1,12} \\\\\n",
    "v_{2,1} & v_{2,2} & ... & v_{2,12} \\\\\n",
    "v_{3,1} & v_{3,2} & ... & v_{3,12} \\end{array}\\right)^T = \\begin{array}{ccc}[v_{1,3}&v_{2,3}&v_{3,3}]\\end{array} $\n",
    "\n",
    "Thus, the hidden layer $\\begin{array}{ccc}[x_1^H&x_2^H&x_1^H]\\end{array} $= $\\begin{array}{ccc}[v_{1,3}&v_{2,3}&v_{3,3}]\\end{array} $\n",
    "\n",
    "<b>NOTE</b> that since $\\mathbf{V}$ is H-by-V in size, each column represents one of the words in our vocabulary. Since <i>uncertainty</i> is the third index in our input matrix, the third column of $\\mathbf{V}$ represents <i>uncertainty</i> in our H-Dimensional space. <b>This H-Dimensional vector is the word embedding we are looking after</b>.\n",
    "\n",
    "###Output Layer###\n",
    "\n",
    "We multiply the hidden layer, $\\begin{array}{ccc}[v_{1,3}&v_{2,3}&v_{3,3}]\\end{array}$ by a 12-by-3 matrix <b>we will call the $\\mathbf{W}$ matrix</b>.\n",
    "\n",
    "$x^h\\mathbf{W}^T=\\begin{array}{ccc}[v_{1,3}&v_{2,3}&v_{3,3}]\\end{array}*\\left(  \\begin{array}{ccc}\n",
    "w_{1,1} & w_{1,2} &w_{1,3} \\\\\n",
    "w_{2,1} & w_{2,2} &w_{2,3} \\\\\n",
    "... \\\\\n",
    "w_{12,1} & w_{12,2} &w_{12,3} \\end{array}\\right)^T = \\begin{array}{cccc}[\\sum_{n=1}^H v_{n,1}*w_{1,n} ;&\\sum_{n=1}^H v_{n,2}*w_{2,n};&...&\\sum_{n=1}^H v_{n,12}*w_{12,n}]\\end{array} $\n",
    "\n",
    "Thus the output layer $\\begin{array}{cccc}[x_1^o&x_2^o&...&x_{12}^0]\\end{array}=\\begin{array}{cccc}[\\sum_{i=1}^H v_{i,1}*w_{1,i} ;&\\sum_{i=1}^H v_{i,2}*w_{2,i};&...&\\sum_{i=1}^H v_{i,12}*w_{12,i}]\\end{array}$\n",
    "\n",
    "###Output Probability###\n",
    "\n",
    "Because we want to be able to predict the words in the context of <i>uncertainty</i> in the sentence, i.e. <i>think</i> and <i>about</i>, we should handle this appropriately. One way is to normalize the values to probabilities by applying the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function). \n",
    "\n",
    "$\\begin{array}{cccc}[\\frac{exp(x_1^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_2^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&...&\\frac{exp(x_{12}^o)}{\\sum_{n=1}^{12} exp(x_n^o)}]\\end{array}$\n",
    "\n",
    "We know we wanted to predict the word <i>think</i> in this example, therefore we have the prediction and the target value.\n",
    "\n",
    "$$\\small{\\begin{array}{l|cccccccccccc|}\n",
    "& \\text{and}  & \\text{uncertainty} & \\text{fears}& \\text{we}& \\text{about}& \\text{constant}& \\text{unemployment}& \\text{negative}& \\text{inflation}& \\text{press}& \\text{think}& \\text{covfefe} \\\\\n",
    "\\hline\n",
    "\\text{Input} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\hline\n",
    "\\text{Output} & \\frac{exp(x_1^o)}{\\sum_{n=1}^{12} exp(x_n^o)} & \\frac{exp(x_2^o)}{\\sum_{n=1}^{12} exp(x_n^o)}& \\frac{exp(x_3^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_4^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_5^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_6^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_7^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_8^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_9^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_{10}^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_{11}^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\frac{exp(x_{12}^o)}{\\sum_{n=1}^{12} exp(x_n^o)}&\\\\\n",
    "\\hline\n",
    "\\text{Target} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{array}}$$\n",
    "\n",
    "## Estimating V and W ##\n",
    "\n",
    "Now I will discuss exactly how we can use these training examples to train the neural network and obtain good estimates fo the word embeddings, matrix $\\mathbf{V}$. \n",
    "\n",
    "To make the estimation derivation clear, I will clarify some of the notation.\n",
    "\n",
    "$f()$ = sigmoid function <br>\n",
    "$x^i$ = Input Vector <br>\n",
    "$x^h$ = Hidden Layer Vector <br>\n",
    "$x^o$ = Output Vector <br>\n",
    "$a^o$ = Output Vector normalized with $f()$ sigmoid function <br>\n",
    "$t^o$ = Target Vector <br>\n",
    "$V$ = total number of unique words <br>\n",
    "$H$ = sizeof hidden dimension <br>\n",
    "\n",
    "Thus the neural network can be summarized as:\n",
    "\n",
    "Input Layer (size 1-by-V): $x^i$  <br>\n",
    "Hidden Layer (size 1-by-H): $x^h=x^i\\mathbf{V^T}$ <br>\n",
    "Output Layer (size 1-by-V): $x^o = x^h \\mathbf{W^T}$ <br>\n",
    "Output Layer Normalized(size 1-by-V): $a^o = f(x^o)$ <br>\n",
    "\n",
    "The loss function we will use to evaluate the model is:\n",
    "\n",
    "$$\n",
    "E_i = \\frac{1}{2} \\sum_{m=-M}^{M} (a^o - t_{m}^o)^2\n",
    "$$\n",
    "\n",
    "The loss function in the Skip-Gram considers one word, $i$, and then forward propogates a value of $a^o$ for that specific word. There will be $M$ words to the left and $M$ words to the right of the word in the sentence. We want the errors to reflect the discrepancy of the model for the word $i$ and the $2*M$ words to the left and right of the word. \n",
    "\n",
    "First, let us understand how the error function $E$ changes with respect to the $\\mathbf{W}$ matrix.\n",
    "\n",
    "### Updating W Matrix ###\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial \\mathbf{W}} &= \\sum_{m=-M}^{M}(a^o-t_m^o)\\frac{\\partial a^o}{\\partial \\mathbf{W}} = \\sum_{m=-M}^{M}(a^o-t_m^o)\\frac{\\partial f(x^h \\mathbf{W^T})}{\\partial \\mathbf{W}}= \\sum_{m=-M}^{M}[(a^o-t_m^o)\\circ f\\prime(x^h \\mathbf{W^T})]\\frac{\\partial (x^h \\mathbf{W^T})}{\\partial \\mathbf{W}} \\\\\n",
    "&=\\sum_{m=-M}^{M}[(a^o-t_m^o)\\circ f\\prime(x^h \\mathbf{W^T})]x^{hT} = sum_{m=-M}^{M}\\delta_m^2 x^{hT} \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $\\circ$ is the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29) and $\\delta_m^2 = [(a^o-t_m^o)\\circ f\\prime(x^h \\mathbf{W^T})]$.\n",
    "\n",
    "Thus, we will update W as follows:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{new} = \\mathbf{W}^{old} - \\alpha [\\sum_{m=-M}^{M}\\delta_m^2 x^{hT}] \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "### Updating V Matrix ###\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial \\mathbf{V}} &= \\sum_{m=-M}^{M}(a^o-t_m^o)\\frac{\\partial a^o}{\\partial \\mathbf{V}} = \\sum_{m=-M}^{M}(a^o-t_m^o)\\frac{\\partial f(x^h \\mathbf{W^T})}{\\partial \\mathbf{V}}= \\sum_{m=-M}^{M}[(a^o-t_m^o)\\circ f\\prime(x^h \\mathbf{W^T})]\\frac{\\partial (x^h \\mathbf{W^T})}{\\partial \\mathbf{V}} \\\\\n",
    "&=\\sum_{m=-M}^{M}\\delta_m^2 \\frac{\\partial (x^h \\mathbf{W^T})}{\\partial \\mathbf{V}}=\\sum_{m=-M}^{M} \\mathbf{W^T} \\delta_m^2 \\frac{\\partial (x^h)}{\\partial \\mathbf{V}}=\\sum_{m=-M}^{M} \\mathbf{W^T} \\delta_m^2 \\frac{\\partial(x^i\\mathbf{V^T})}{\\partial \\mathbf{V}} = \\sum_{m=-M}^{M} \\mathbf{W^T} \\delta_m^2 x^{iT}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Thus, we will update V as follows:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\mathbf{V}^{new} = \\mathbf{V}^{old} - \\alpha [\\sum_{m=-M}^{M} \\mathbf{W^T} \\delta_m^2 x^{iT}] \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "## Implementation in Python ##\n",
    "\n",
    "First, let's load the relevant libraries and create our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "\n",
    "docs = [\"we think uncertainty about unemployment\",\n",
    "\t\t\"uncertainty and fears about inflation\",\n",
    "\t\t\"we think fears about unemployment\",\n",
    "\t\t\"we think fears and uncertainty about inflation and unemployment\",\n",
    "\t\t\"constant negative press covfefe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to find the unique words in our documents and then generate a one-hot encoded vector of length 12 for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_split = map(lambda x: x.split(),docs)\n",
    "docs_words = list(itertools.chain(*docs_split))\n",
    "words = np.unique(docs_words)\n",
    "\n",
    "vectors = np.eye(words.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the window size of the context, $M$. That is, how many words to look left and right for each word in order to traing the matrices. We will also define the matrices $\\mathbf{V}$ and $\\mathbf{W}$ as well as the number of dimensions in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 2\n",
    "H = 3\n",
    "\n",
    "V = np.random.randn(H,words.shape[0])\n",
    "W = np.random.randn(words.shape[0],H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in each sentence, we need to know the context. We will create a function which takes in a sentence, in the form of a list of words, and then return the $M$ words to the left and right of each word in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(word_list):\n",
    "\tsamples = []\n",
    "\tfor word_ind in range(len(word_list)):\n",
    "\t\tcontext_inds = range(max(0,word_ind-M),\n",
    "\t\t\t\t\t\t\tmin(word_ind+M+1,len(word_list)))\n",
    "\t\tcontext_inds.remove(word_ind)\n",
    "\t\tcontext = [word_list[el] for el in context_inds]\n",
    "\t\tsamples.append((word_list[word_ind],context))\n",
    "\treturn samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate all the training examples, apply <b>get_context</b> to each sentence in <i>docs_split</i>, and then converting it to one list using itertools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = list(itertools.chain(*map(get_context,docs_split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we will get to the meat of the code. We will run 10,000 <b>epochs</b>, which means looping through each word in each sentence 10,000 times. For each word, we will <b>forward propogate</b> the one-hot encoded vector and obtain a probability distribution over all 12 words in our vocabular. Then, for each word in the context, compute the error. Lastly, we will sum up the errors and then update the $\\mathbf{V}$ and $\\mathbf{W}$ matrices. \n",
    "\n",
    "Along the way, I will update the log likehood function, to ensure we are maximizing the probabilities. \n",
    "\n",
    "Also, I use a linear learning rate, so that each iteration it gets linearly closer to 0. I chose the default value of 2.5% but this is a <b>hyper parameter</b> (along with the H-Dimension in the hidden layer) which should be messed around with to get best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(vector, deriv=False):\n",
    "\tif deriv:\n",
    "\t\treturn sigma(vector)*(1-sigma(vector))\n",
    "\telse:\n",
    "\t\treturn np.exp(vector)/np.exp(vector).sum()\n",
    "\n",
    "log_likelihood = np.array([])\n",
    "epochs = 10000\n",
    "learning_rate = 0.001\n",
    "tolerance = 0.001\n",
    "discount = float(learning_rate)/epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tlikelihood = 0\n",
    "\tfor example in training:\n",
    "\t\t# Forward propogate word\n",
    "\t\tinput_index = np.where(words==example[0])[0][0]\n",
    "\t\tl_input = vectors[input_index]\n",
    "\t\tl_hidden = np.dot(V,l_input)\n",
    "\t\tl_output = np.dot(W,l_hidden)\n",
    "\t\tl_output_a = sigma(l_output)\n",
    "\t\terrors = np.zeros(words.shape[0])\n",
    "\t\t# Compute the error for each word in context window\n",
    "\t\tfor context in example[1]:\n",
    "\t\t\toutput_index = np.where(words==context)[0][0]\n",
    "\t\t\tl_target= vectors[output_index]\n",
    "\t\t\terrors += (l_output_a-l_target)\n",
    "\t\t# Update the weights of V and W matrices\n",
    "\t\tdelta2 = errors*sigma(l_output,True)\n",
    "\t\tW -= learning_rate*np.outer(delta2,l_hidden)\n",
    "\t\tV -= learning_rate*np.outer(np.dot(W.T,delta2),l_input)\n",
    "\t\tlikelihood+=sum(map(np.log,l_output_a))\n",
    "\tlog_likelihood=np.append(log_likelihood,likelihood)\n",
    "\tlearning_rate -= discount\n",
    "\tif epoch<2: continue\n",
    "\tif (abs(likelihood-log_likelihood[-2])<tolerance):\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot out the word embeddings from the matrix $\\mathbf{V}$ along with the log likelihood function. Since the initialized $\\mathbf{V}$ and $\\mathbf{W}$ matrices were initialized to be random numbers from a normal distribution, you will get different results. Though the log likelihood and the proximity of words should be more or less similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1,projection=\"3d\")\n",
    "ax.scatter(V[0],V[1],V[2], alpha=0.3)\n",
    "for i,txt in enumerate(words):\n",
    "\tax.text(V[0][i],V[1][i],V[2][i],txt, size=10)\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(log_likelihood)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"markdown/scatter.png\">\n",
    "<img src=\"markdown/ll.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "### Author: Paul Soto \t\t  ###\n",
    "### \t\tpaul.soto@upf.edu ###\n",
    "#\t\t\t\t\t\t\t\t#\n",
    "# This file is a script to run ##\n",
    "# a Skip Gram Model using a toy #\n",
    "# sample of documents. While ####\n",
    "# negative sampling should be ###\n",
    "# used to train the neural ######\n",
    "# network, I use gradient #######\n",
    "# descent to focus on the #######\n",
    "# architecture rather than the ##\n",
    "# optimal estimation ############\n",
    "#################################\n",
    "\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# M is the number of words to look (on one side) of each word for the context\n",
    "M = 2\n",
    "# H is the dimension of the hidden layer\n",
    "H = 3\n",
    "\n",
    "def sigma(vector, deriv=False):\n",
    "\t\"\"\"\n",
    "\tThis function returns a vector evaluated using the sigmoid function\n",
    "\n",
    "\tvector: numpy array of real values\n",
    "\tderiv: if True, evaluate first derivate of sigmoid \n",
    "\t\"\"\"\n",
    "\tif deriv:\n",
    "\t\treturn sigma(vector)*(1-sigma(vector))\n",
    "\telse:\n",
    "\t\treturn np.exp(vector)/np.exp(vector).sum()\n",
    "\n",
    "def get_context(word_list):\n",
    "\t\"\"\"\n",
    "\tThis function returns the 2*M words in the context of each word in \n",
    "\tthe list\n",
    "\n",
    "\tword_list: List of words\n",
    "\tM: global variable of the window size\n",
    "\t\"\"\"\n",
    "\tsamples = []\n",
    "\tfor word_ind in range(len(word_list)):\n",
    "\t\tcontext_inds = range(max(0,word_ind-M),\n",
    "\t\t\t\t\t\t\tmin(word_ind+M+1,len(word_list)))\n",
    "\t\tcontext_inds.remove(word_ind)\n",
    "\t\tcontext = [word_list[el] for el in context_inds]\n",
    "\t\tsamples.append((word_list[word_ind],context))\n",
    "\treturn samples\n",
    "\n",
    "docs = [\"we think uncertainty about unemployment\",\n",
    "\t\t\"uncertainty and fears about inflation\",\n",
    "\t\t\"we think fears about unemployment\",\n",
    "\t\t\"we think fears and uncertainty about inflation and unemployment\",\n",
    "\t\t\"constant negative press covfefe\"]\n",
    "\n",
    "\n",
    "# Split each document into a list of words\n",
    "docs_split = map(lambda x: x.split(),docs)\n",
    "docs_words = list(itertools.chain(*docs_split))\n",
    "\n",
    "# Find unique words across all documents\n",
    "words = np.unique(docs_words)\n",
    "\n",
    "# Generate a one hot encoded vector for each unique word\n",
    "vectors = np.eye(words.shape[0])\n",
    "\n",
    "# Initiate randomly V and W matrices\n",
    "V = np.random.randn(H,words.shape[0])\n",
    "W = np.random.randn(words.shape[0],H)\n",
    "\n",
    "# Create list of all training examples\n",
    "training = list(itertools.chain(*map(get_context,docs_split)))\n",
    "\n",
    "log_likelihood = np.array([])\n",
    "epochs = 10000\n",
    "learning_rate = 0.001\n",
    "tolerance = 0.001\n",
    "discount = float(learning_rate)/epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tlikelihood = 0\n",
    "\tfor example in training:\n",
    "\t\t# Forward propogate word\n",
    "\t\tinput_index = np.where(words==example[0])[0][0]\n",
    "\t\tl_input = vectors[input_index]\n",
    "\t\tl_hidden = np.dot(V,l_input)\n",
    "\t\tl_output = np.dot(W,l_hidden)\n",
    "\t\tl_output_a = sigma(l_output)\n",
    "\t\terrors = np.zeros(words.shape[0])\n",
    "\t\t# Compute the error for each word in context window\n",
    "\t\tfor context in example[1]:\n",
    "\t\t\toutput_index = np.where(words==context)[0][0]\n",
    "\t\t\tl_target= vectors[output_index]\n",
    "\t\t\terrors += (l_output_a-l_target)\n",
    "\t\t# Update the weights of V and W matrices\n",
    "\t\tdelta2 = errors*sigma(l_output,True)\n",
    "\t\tW -= learning_rate*np.outer(delta2,l_hidden)\n",
    "\t\tV -= learning_rate*np.outer(np.dot(W.T,delta2),l_input)\n",
    "\t\tlikelihood+=sum(map(np.log,l_output_a))\n",
    "\tlog_likelihood=np.append(log_likelihood,likelihood)\n",
    "\tlearning_rate -= discount\n",
    "\tif epoch<2: continue\n",
    "\tif (abs(likelihood-log_likelihood[-2])<tolerance):\n",
    "\t\tbreak\n",
    "Plot out word embeddings and log-likelihood function\n",
    "# Plot out word embeddings and log-likelihood function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1,projection=\"3d\")\n",
    "ax.scatter(V[0],V[1],V[2], alpha=0.3)\n",
    "for i,txt in enumerate(words):\n",
    "\tax.text(V[0][i],V[1][i],V[2][i],txt, size=10)\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(log_likelihood)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this was useful for you. Any questions or feedback please forward to my email paul.soto@upf.edu or message me on GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
